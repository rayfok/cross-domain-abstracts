{"paper_sha": "8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48", "model": "gpt-4", "original": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "rewritten": "We present the development of GPT-4, a large-scale, multimodal model that functions similarly to the integrated processing of sensory information in the human brain. GPT-4 can accept both visual and textual inputs, akin to the simultaneous processing of visual and linguistic stimuli in the brain, and generate textual outputs. Although not as proficient as humans in many real-world tasks, GPT-4 demonstrates human-level performance in specific professional and academic benchmarks, such as passing a simulated bar exam with a score comparable to the top 10% of test-takers.\n\nGPT-4 is based on the Transformer architecture, which shares similarities with the neural mechanisms of information processing and prediction in the brain. It is pre-trained to predict the next element in a sequence, much like how the brain anticipates upcoming stimuli based on prior experiences. The post-training alignment process enhances GPT-4's performance in terms of factuality and adherence to desired behavior, reflecting the brain's ability to adapt and refine its predictions over time.\n\nA crucial aspect of this project involved developing infrastructure and optimization methods that function consistently across various scales, reminiscent of the brain's capacity to process information at different levels of complexity. This enabled us to accurately predict certain aspects of GPT-4's performance using models trained with significantly less computational resources, analogous to the brain's ability to generalize and transfer learning across different contexts."}
{"paper_sha": "73f85c0aa4b0ae53749160b204a1d8a1e2ef97f1", "model": "gpt-4", "original": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "rewritten": "In recent years, artificial intelligence (AI) researchers have made significant advancements in the development of large-scale neural network models, which have demonstrated impressive capabilities in various domains and tasks. These advancements have implications for our understanding of learning and cognition in both artificial and biological systems. In this paper, we investigate an early version of OpenAI's GPT-4, a large language model (LLM) that is part of a new generation of AI models, including ChatGPT and Google's PaLM, which exhibit more general intelligence than their predecessors.\n\nGPT-4, which was trained using a massive amount of computational resources and data, demonstrates remarkable mastery of language and problem-solving abilities across diverse fields such as mathematics, coding, vision, medicine, law, and psychology. Its performance is strikingly close to human-level in many tasks, often surpassing previous models like ChatGPT. This suggests that GPT-4 could be considered an early, albeit incomplete, example of an artificial general intelligence (AGI) system, which may have implications for understanding the neural mechanisms underlying human cognition.\n\nIn our investigation, we focus on identifying the limitations of GPT-4 and discuss the challenges that must be addressed to develop deeper and more comprehensive AGI systems. This may involve exploring new paradigms beyond the current next-word prediction approach used in LLMs. We also reflect on the societal impact of these rapidly advancing AI technologies and suggest future research directions that may be of interest to both AI and neuroscience researchers.\n\nBy presenting our findings in the context of neuroscience, we hope to bridge the gap between AI and neuroscience research, fostering interdisciplinary collaboration and providing valuable insights into the neural basis of learning and cognition."}
{"paper_sha": "57e849d0de13ed5f91d086936296721d4ff75a75", "model": "gpt-4", "original": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "rewritten": "We present LLaMA, a series of foundational language models with a range of complexity, from 7 billion to 65 billion parameters. These models can be likened to artificial neural networks, which emulate the human brain's ability to process and understand language. We train our models on vast amounts of linguistic data, demonstrating that it is feasible to develop cutting-edge models using only publicly accessible datasets, without relying on exclusive and restricted resources. Specifically, LLaMA-13B surpasses the performance of GPT-3 (175 billion parameters) in the majority of assessments, while LLaMA-65B competes with the top-performing models, Chinchilla-70B and PaLM-540B. We make all our models available to the research community, offering potential applications in fields such as neuroscience, where understanding language processing and cognition is of great importance."}
{"paper_sha": "e55695dfe6cde42ee195aa6672fe720ec92ee8c3", "model": "gpt-4", "original": "We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (<50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.", "rewritten": "In this study, we introduce a novel neural network architecture, ControlNet, designed to modulate the activity of pretrained large-scale diffusion models, akin to how neuromodulatory systems influence neuronal activity in the brain. ControlNet is capable of learning task-specific conditions in an end-to-end manner, much like how neural circuits adapt to specific sensory or motor tasks. Importantly, this learning is robust even with small training datasets (<50k), reminiscent of the brain's ability to learn from limited experiences.\n\nThe training process for ControlNet is as efficient as fine-tuning a diffusion model, allowing for the possibility of training on personal devices. This feature parallels the localized learning observed in specific brain regions. However, when powerful computational resources are available, ControlNet can also scale to handle large quantities of data (millions to billions), similar to the distributed processing capabilities of the nervous system.\n\nWe demonstrate that large diffusion models, such as Stable Diffusion, can be augmented with ControlNets to enable conditional inputs, such as edge maps, segmentation maps, and keypoints. These additional inputs can be compared to the various types of sensory information processed by the brain. By incorporating ControlNet into diffusion models, we aim to enrich the methods for controlling these models and further facilitate applications relevant to both computer science and neuroscience research."}
{"paper_sha": "1be6eef5e3f2cc6c1ee0a5c4031af4abd4badb3a", "model": "gpt-4", "original": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.", "rewritten": "Large-scale artificial neural networks, known as large language models (LLMs), have shown impressive abilities in understanding and generating human language across various fields, including medicine. In this study, we evaluate GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model, not specifically trained or designed for medical tasks, akin to how the human brain is not specifically wired for medical knowledge but can acquire it through learning.\n\nOur evaluation includes two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States, and the MultiMedQA suite of benchmark datasets. We also examine how GPT-4 performs on test questions containing both text and images, probe for memorization of content during training, and study probability calibration, a crucial aspect in high-stakes fields like medicine.\n\nOur results show that GPT-4, without any specialized prompt crafting, surpasses the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). Furthermore, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating an improved ability to predict the likelihood that its answers are correct, similar to how a neuroscientist might predict the outcome of an experiment based on prior knowledge.\n\nWe also explore GPT-4's behavior qualitatively through a case study, illustrating its capacity to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. This is similar to how the human brain can adapt and generate new solutions based on existing knowledge. We discuss the implications of our findings for potential uses of GPT-4 in medical education, assessment, and clinical practice, while considering the challenges of accuracy and safety, much like how neuroscientists weigh the benefits and risks of novel treatments or interventions."}
{"paper_sha": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "model": "gpt-4", "original": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "rewritten": "In the study of neural language processing, large-scale language models (analogous to neural networks) have demonstrated impressive abilities to learn and perform new tasks based on minimal examples or textual instructions. However, these models paradoxically struggle with fundamental functions, such as arithmetic or factual retrieval, which smaller and simpler models can perform efficiently. In this paper, we demonstrate that these language models can improve their performance by learning to use external tools through simple interfaces, similar to how neurons communicate via synapses.\n\nWe present Toolformer, a model designed to determine which external tools (akin to specialized brain regions) to utilize, when to engage them, what information to exchange, and how to integrate the resulting data into its ongoing language processing tasks. This learning process occurs in a self-supervised manner, analogous to the brain's ability to learn from experience, requiring only a small number of demonstrations for each external tool.\n\nThe external tools incorporated in Toolformer include a calculator, a question-and-answer system, two distinct search engines, a translation system, and a calendar. These tools can be seen as specialized modules in a larger cognitive system, similar to how different brain regions are specialized for specific functions. Toolformer demonstrates significant improvement in zero-shot performance (i.e., the ability to generalize to new tasks without prior training) across various tasks, often rivaling the performance of much larger models, without compromising its core language processing abilities.\n\nThis research provides insights into how neural language models can leverage external resources to enhance their performance, potentially informing the development of more efficient and adaptive artificial neural networks that mimic the human brain's ability to learn and adapt."}
{"paper_sha": "873a581320d928249609d3c07229d5af182a379c", "model": "gpt-4", "original": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.", "rewritten": "Recent advancements in large-scale neural networks have led to the development of large language models (LLMs) capable of performing various natural language processing (NLP) tasks without prior training on specific data, a phenomenon known as zero-shot learning. This concept may be familiar to neuroscientists, as it somewhat resembles the brain's ability to generalize and adapt to new information without explicit instruction. One such LLM, ChatGPT, has garnered interest due to its capacity to generate high-quality responses to human input and self-correct based on subsequent interactions, reminiscent of the way neural circuits in the brain can adapt and refine their responses to stimuli.\n\nHowever, it remains unclear whether ChatGPT can serve as a generalist model, performing a wide range of NLP tasks in a zero-shot manner. In this study, we investigate ChatGPT's zero-shot learning capabilities by evaluating its performance on 20 widely-used NLP datasets, covering 7 representative task categories. Our extensive empirical analysis reveals both the strengths and limitations of ChatGPT's current iteration.\n\nWe observe that ChatGPT excels in tasks that require reasoning abilities, such as arithmetic reasoning, which may be of interest to neuroscientists studying cognitive processes and problem-solving. However, it struggles with specific tasks like sequence tagging, suggesting that further refinement and adaptation may be necessary to improve its performance in these areas. We also offer a detailed examination of ChatGPT's performance through qualitative case studies, providing valuable insights for researchers in both computer science and neuroscience fields."}
{"paper_sha": "fdcb65bda2f2ff57763c640628540848f16ad9bb", "model": "gpt-4", "original": "DeepMind presented remarkably accurate predictions at the recent CASP14 protein structure prediction assessment conference. We explored network architectures incorporating related ideas and obtained the best performance with a three-track network in which information at the 1D sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging X-ray crystallography and cryo-EM structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short circuiting traditional approaches which require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.", "rewritten": "DeepMind recently showcased highly precise predictions at the CASP14 protein structure prediction assessment conference, a significant development for understanding the complex 3D folding patterns of proteins that are crucial in various neurological processes. In our study, we investigated network architectures that incorporate related concepts and achieved optimal performance using a three-track network. This approach successively transforms and integrates information at the 1D sequence level (amino acid sequence), the 2D distance map level (proximity of amino acids), and the 3D coordinate level (spatial organization of the protein).\n\nThe three-track network generates structure predictions with accuracies nearing those of DeepMind in CASP14, allowing for rapid solutions to challenging X-ray crystallography and cryo-electron microscopy (cryo-EM) structure modeling problems, which are essential in revealing the molecular architecture of proteins involved in neural signaling and synaptic transmission. Additionally, the network provides insights into the functions of proteins with currently unknown structures that may play a role in various neurological pathways.\n\nFurthermore, our network enables the swift generation of accurate protein-protein complex models using only sequence information, bypassing traditional methods that typically require separate modeling of individual subunits followed by docking. This advancement has the potential to accelerate our understanding of protein interactions within the nervous system. We make this method available to the scientific community to expedite biological research, particularly in the field of neuroscience."}
{"paper_sha": "be7b764fe1c9c32cbe349bde1fbb19321fd1d71c", "model": "gpt-4", "original": "Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre-training paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.", "rewritten": "In the field of neuroscience, the study of visual recognition often involves understanding how the brain processes and generalizes information from limited input data. Similarly, in computer science, deep neural networks are designed to learn generalized representations from limited training samples in low-data regimes. Recent developments in CLIP-based methods, which use contrastive language-image pre-training, have shown promising results in few-shot learning tasks, where the neural network must learn from a small number of examples. This raises the question of whether incorporating more diverse pre-training knowledge can further enhance few-shot representation learning.\n\nIn this paper, we introduce CaFo, a Cascade of Foundation models that combines various pre-training paradigms to improve few-shot learning performance. CaFo brings together the language-contrastive knowledge from CLIP, vision-contrastive knowledge from DINO, vision-generative knowledge from DALL-E, and language-generative knowledge from GPT-3. The CaFo model operates through a three-step process: 'Prompt, Generate, then Cache'. \n\nFirst, we utilize GPT-3, a powerful language model, to generate textual inputs that provide rich linguistic semantics for prompting CLIP, analogous to how the brain processes language and visual information together. Next, we employ DALL-E, a vision-generative model, to create synthetic images that expand the few-shot training data without requiring additional human effort, similar to the brain's ability to generate mental images based on limited input. Finally, we introduce a learnable cache model that adaptively blends the predictions from CLIP and DINO, allowing CaFo to fully exploit the potential of different pre-training methods and combine them for state-of-the-art few-shot classification performance.\n\nThis interdisciplinary approach to few-shot learning may provide insights for neuroscientists studying the brain's ability to process and generalize visual information from limited input data. The code for CaFo is available at https://github.com/ZrrSkywalker/CaFo."}
{"paper_sha": "0570e8fc8b02e7eb66e798b00726fba0592ea90f", "model": "gpt-4", "original": null, "rewritten": null}
{"paper_sha": "c57293882b2561e1ba03017902df9fc2f289dea2", "model": "gpt-4", "original": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.", "rewritten": "Contrastive models, such as CLIP, have demonstrated their ability to learn robust representations of visual stimuli, capturing both the semantic content and stylistic features. In the context of neuroscience, this can be compared to how the brain processes and encodes visual information. To utilize these representations for image generation, we suggest a two-stage model: a prior that generates a CLIP image embedding based on a text caption (akin to a mental image evoked by language), and a decoder that creates an image conditioned on the image embedding (similar to the brain's reconstruction of visual stimuli).\n\nWe demonstrate that by explicitly generating image representations, there is an increase in image diversity with minimal loss in photorealism and caption similarity. This can be seen as analogous to the brain's ability to generate diverse mental images while maintaining the core semantic and stylistic features. Our decoders, conditioned on image representations, can also produce variations of an image that preserve both its semantics and style, while altering non-essential details not present in the image representation, much like the brain's capacity for visual imagination and modification.\n\nFurthermore, the joint embedding space of CLIP allows for language-guided image manipulations in a zero-shot fashion, which can be compared to the brain's ability to modify mental images based on linguistic input. We employ diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior. We find that diffusion models are more computationally efficient and produce higher-quality samples, which may provide insights into the efficiency and quality of neural processes underlying visual imagery and reconstruction."}
{"paper_sha": "cf1f26e7cbed3958b3c2870656568c299fece6e3", "model": "gpt-4", "original": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.", "rewritten": "In this study, we assessed the capabilities of a large-scale artificial neural network, known as ChatGPT, in the context of medical knowledge and reasoning. The ChatGPT model, which is analogous to the interconnected neurons in the human brain, was tested on the United States Medical Licensing Exam (USMLE). The USMLE is a series of three assessments - Step 1, Step 2CK, and Step 3 - that evaluate medical students' understanding of fundamental concepts and clinical decision-making skills.\n\nRemarkably, without any specialized training or reinforcement learning, ChatGPT was able to perform at or near the passing threshold for all three exams. Furthermore, the model exhibited a high degree of coherence and depth in its explanations, akin to the efficient information processing observed in the human nervous system.\n\nThese findings imply that large-scale artificial neural networks, such as ChatGPT, hold promise in supporting medical education and may even contribute to clinical decision-making processes. For neuroscience researchers, this study highlights the potential of artificial neural networks in emulating human cognitive abilities and their potential applications in various domains, including medicine."}
{"paper_sha": "20c1cb888c4f93003774cc864ff8dc443ec03568", "model": "gpt-4", "original": "Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.", "rewritten": "Recent advancements in large-scale artificial neural networks, known as large language models (LLMs), have led to the development of highly capable models such as OpenAI's ChatGPT. These models have demonstrated exceptional performance in various tasks, including question answering, essay composition, and code generation, which can be compared to the versatility of the human brain in processing language. However, their effectiveness in the healthcare sector, particularly in processing and understanding complex clinical texts, remains uncertain.\n\nIn this study, we explore the potential of ChatGPT to assist in clinical text mining, a process that can be likened to the brain's ability to extract meaningful information from unstructured data. We focus on two key tasks: biological named entity recognition (identifying specific biological terms within the text) and relation extraction (determining the relationships between these terms). Our initial results indicate that using ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API.\n\nTo address these limitations, we propose a new training paradigm that involves generating a large amount of high-quality synthetic data with labels using ChatGPT and fine-tuning a local model for the downstream task. This approach can be compared to the process of synaptic plasticity in the brain, where neural connections are refined based on new experiences. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score (a measure of accuracy) from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task.\n\nMoreover, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining, potentially enabling more effective extraction of valuable information from complex healthcare data, akin to the information processing capabilities of the human brain."}
{"paper_sha": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "model": "gpt-4", "original": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.", "rewritten": "Abstract: Background In the field of natural language processing, the Chat Generative Pre-trained Transformer (ChatGPT) is a highly advanced model with 175 billion parameters, capable of generating conversation-style responses to user input. Objective This study aimed to assess the performance of ChatGPT in answering questions related to the United States Medical Licensing Examination Step 1 and Step 2 exams, which are relevant to the medical aspects of neuroscience, and to analyze the interpretability of the generated responses. Methods We utilized two sets of multiple-choice questions to evaluate ChatGPT's performance in the context of Step 1 and Step 2 exams. The first set was sourced from AMBOSS, a popular question bank for medical students that also provides data on question difficulty and user performance. The second set consisted of the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT's performance was compared to two other large language models, GPT-3 and InstructGPT. Each ChatGPT response was assessed based on three qualitative metrics: the logical justification of the chosen answer, the presence of information internal to the question, and the presence of information external to the question. Results ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102) in the AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2 data sets, respectively. It outperformed InstructGPT by an average of 8.15% across all data sets, while GPT-3 performed similarly to random chance. ChatGPT's performance significantly decreased as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. Logical justification for ChatGPT's answer selection was present in 100% of outputs in the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of external information was 44.5% and 27% lower for incorrect answers compared to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT represents a significant advancement in natural language processing models for medical question answering. By"}
{"paper_sha": "293499319bdd460cb3fca1f0f5eb330e64bf3ff9", "model": "gpt-4", "original": "ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.", "rewritten": "ChatGPT, a language model, demonstrates promising potential in machine translation (MT), a process akin to the brain's translation of neural signals into meaningful information. Previous research has compared ChatGPT's performance to commercial systems in high-resource languages, finding limitations in complex tasks such as low-resource and distant-language-pair translation. These studies, however, often employ simplistic prompts that may not fully capture ChatGPT's capabilities. In this report, we investigate ChatGPT's translation ability by examining the influence of temperature, task information, and domain information, drawing parallels to the modulatory effects of various factors on neuronal activity.\n\nWe propose two straightforward yet effective prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). Our findings indicate that: 1) ChatGPT's performance is heavily influenced by temperature, with lower temperatures generally yielding better results, similar to the impact of temperature on neural signal transmission; 2) Emphasizing task information enhances ChatGPT's performance, particularly in complex MT tasks, akin to how task-specific neuronal activation patterns improve cognitive performance; 3) Incorporating domain information improves ChatGPT's generalization ability and performance in specific domains, comparable to how domain-specific knowledge affects neural processing; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially mitigated by our proposed prompts but warrant further attention from the MT/NLP community, much like the need to understand and address hallucinations in neuroscience.\n\nWe also explore the effects of advanced in-context learning strategies, drawing an intriguing parallel to the brain's learning mechanisms, and observe that the powerful chain-of-thought prompt leads to word-by-word translation behavior, resulting in significant translation degradation. This study aims to bridge the gap between computer science and neuroscience, providing insights into how artificial language models can be better understood and optimized through a neuroscientific lens."}
{"paper_sha": "1b492746ee3a304a13950cad1a59861b9ee44645", "model": "gpt-4", "original": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and self-supervised pretraining to generative modeling methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream industries, such as education and creativity content. Finally, we discuss the challenges currently faced and present an outlook on how generative AI might evolve in the near future.", "rewritten": "As the popularity of ChatGPT increases, AI-generated content (AIGC) has gained significant attention for its potential to analyze and create various forms of content, such as text and images. This phenomenon can be compared to the way neural networks in the brain process and generate information. In the context of AI transitioning from analysis to creation, ChatGPT, powered by the GPT-4 language model, represents just one aspect of numerous AIGC tasks. Researchers in the field of neuroscience may be interested in understanding the limits of ChatGPT and whether future iterations, such as GPT-5, can unify all AIGC tasks for diverse content creation.\n\nTo address this question, a thorough review of existing AIGC tasks is necessary. Our work provides an initial examination of AIGC, covering its techniques and applications. Modern generative AI, similar to how the brain relies on various neural pathways and mechanisms, depends on multiple technical foundations. These include model architecture, self-supervised pretraining, and generative modeling methods reminiscent of neural plasticity (e.g., GAN and diffusion models).\n\nAfter presenting the foundational techniques, this work delves into the technological development of various AIGC tasks based on their output type, such as text, images, videos, and 3D content. This exploration helps illustrate the full potential of ChatGPT's future capabilities in a context that is relevant to neuroscience researchers. Additionally, we summarize significant applications of these techniques in mainstream industries, such as education and creative content production, which may have parallels to cognitive processes and learning.\n\nLastly, we discuss the challenges currently faced by generative AI and provide an outlook on how this field might evolve in the near future, offering valuable insights for researchers in neuroscience interested in the intersection of AI and brain function."}
{"paper_sha": "12bc6df26d3cdfdb27f20cbb95c8b2c45e4f1c3a", "model": "gpt-4", "original": "We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.", "rewritten": "We present a novel approach for analyzing 3D point cloud data, Point-NN, which can be likened to the functioning of neural networks in the brain. This Non-parametric Network is composed of entirely non-learnable components, similar to the fixed connections between neurons: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, along with trigonometric functions. Remarkably, Point-NN demonstrates strong performance on various 3D tasks without the need for parameters or training, even surpassing some fully trained models.\n\nDrawing inspiration from the adaptability of neural networks, we propose two extensions for Point-NN. First, it can serve as a foundational architecture for constructing Parametric Networks by simply adding linear layers on top, similar to the way synaptic connections can be strengthened in the brain. Due to its robust non-parametric base, the resulting Point-PN achieves an optimal balance between performance and efficiency, requiring only a few learnable parameters. Second, Point-NN can be integrated as a plug-and-play module for existing trained 3D models during inference, analogous to incorporating new information into a neural network. This allows Point-NN to capture complementary geometric knowledge and improve existing methods for different 3D benchmarks without the need for re-training.\n\nWe believe our work can provide valuable insights for the neuroscience community in understanding 3D point cloud data using non-parametric methods. The code for our approach is available at https://github.com/ZrrSkywalker/Point-NN."}
{"paper_sha": "12cac86b9cb5557d7f75b6fbcab0bac40b5f7995", "model": "gpt-4", "original": "The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.", "rewritten": "The field of explainable artificial intelligence (XAI) focuses on developing methods to elucidate the underlying mechanisms of complex machine learning systems, similar to how neuroscientists strive to understand the neural basis of cognition and behavior. XAI aims to provide clear explanations for the predictions, recommendations, and decisions made by these intricate systems, which are becoming increasingly prevalent in academic libraries. These libraries now offer resources, services, and collections generated and delivered by machine learning algorithms, analogous to the way brain networks process and transmit information.\n\nConsidering the importance of transparency and validation in neuroscience research, academic libraries should embrace XAI as a means to ensure the reliability and credibility of these machine learning-based resources. By adopting XAI, libraries can not only verify and validate the resources they provide but also advocate for public policies that promote transparency and accountability in artificial intelligence, ultimately benefiting the scientific community and the broader public. This approach aligns with the values and objectives of neuroscience research, which emphasizes the importance of understanding the inner workings of complex systems to improve their overall reliability and applicability."}
{"paper_sha": "92f19090599910af1b1c9ed2b318abc0adea0527", "model": "gpt-4", "original": "As a natural language assistant, ChatGPT is capable of performing various tasks, including but not limited to article generation, code completion, and data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evaluation, exhibiting the capability of mimicking human preferences. To further explore ChatGPT's potential in this regard, a study is conducted to assess its ability to rank content. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. ChatGPT is then instructed to rank the responses generated by these models. The results on the test set show that ChatGPT's ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that ChatGPT's zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.", "rewritten": "Abstract: ChatGPT, a natural language assistant, functions similarly to the way the human brain processes and generates language, enabling it to perform various tasks relevant to researchers in neuroscience, such as generating articles, completing code, and analyzing data. Like the neural networks in the brain, ChatGPT demonstrates a remarkable level of accuracy and reliability in content evaluation, exhibiting the ability to mimic human preferences. To investigate the potential of ChatGPT in the context of neuroscience, a study is conducted to assess its capacity to rank content. A test set comprising prompts is created, covering a diverse range of use cases relevant to the field, and five models generate corresponding responses. ChatGPT is then tasked with ranking the responses produced by these models. The results indicate that ChatGPT's ranking preferences align with human preferences to a certain extent. This preliminary finding suggests that ChatGPT's zero-shot ranking capability could be employed to decrease the burden of annotation in various ranking tasks, potentially benefiting neuroscience research endeavors."}
{"paper_sha": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "model": "gpt-4", "original": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. 1", "rewritten": "Recent advancements in language-vision models, such as CLIP and DALL-E, have demonstrated the potential for training on extensive sets of noisy image-text data without relying on costly, accurate labels typically used in unimodal supervised learning for vision tasks. These models exhibit strong text-guided image generation and transfer to downstream tasks, while also performing remarkably well in zero-shot classification, showing considerable resilience to out-of-distribution data. This development is reminiscent of the way the human brain processes multi-modal information, integrating visual and linguistic cues to understand and generate complex stimuli.\n\nFollowing this trend, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo, and Imagen have made further improvements. However, analyzing the training and capabilities of such models necessitates datasets containing billions of image-text pairs. Until now, no datasets of this magnitude have been openly accessible for the broader research community, limiting the potential for interdisciplinary research, particularly in the field of neuroscience.\n\nTo address this issue and facilitate research on large-scale multi-modal models across various disciplines, including neuroscience, we present LAION-5B, a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, with 2.32 billion containing English language text. This dataset allows for successful replication and fine-tuning of foundational models like CLIP, GLIDE, and Stable Diffusion. Moreover, it enables further experiments that can draw upon the expertise of neuroscientists, potentially leading to a deeper understanding of human multi-modal processing and the development of more advanced artificial systems.\n\nIn addition to the dataset, we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. This comprehensive resource package aims to foster collaboration and innovation across various research fields, including neuroscience, and contribute to the development of more robust and efficient multi-modal models."}
{"paper_sha": "cff26bda86237d113ed01c812ad8bedd0afbe070", "model": "gpt-4", "original": "The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework (\"DeID-GPT\") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.", "rewritten": "The increasing digitalization of healthcare has enabled the efficient sharing and re-utilization of medical data, including patient records and neuroscientific research findings. However, this progress also raises concerns regarding confidentiality and privacy. To address these concerns, the Health Insurance Portability and Accountability Act (HIPAA) requires the removal of identifiable information before disseminating medical records. Consequently, there is a growing need for effective and efficient methods to de-identify medical data, particularly in free-text forms, such as clinical notes and research manuscripts.\n\nPrevious computer-assisted de-identification approaches, including rule-based and learning-based methods, have been developed but often lack generalizability and require fine-tuning for specific scenarios, limiting their widespread use. The recent advancements in large language models (LLMs), such as ChatGPT and GPT-4, have demonstrated their potential in processing text data within the medical domain, including neuroscience research. These models can contribute to privacy protection by identifying confidential information using their robust named entity recognition (NER) capabilities.\n\nIn this study, we developed a novel GPT4-enabled de-identification framework (\"DeID-GPT\") to automatically identify and remove identifying information from medical text data, including those related to neuroscience research. Our DeID-GPT outperformed existing de-identification methods, demonstrating the highest accuracy and remarkable reliability in masking private information while preserving the original structure and meaning of the text.\n\nThis research is among the first to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, providing valuable insights for further studies and the development of LLM-based solutions in healthcare, including neuroscience applications. The codes and benchmarking data information for our DeID-GPT framework can be accessed at https://github.com/yhydhx/ChatGPT-API."}
{"paper_sha": "fb47aa3c541fc2a9b340c9d2a3572860811767d6", "model": "gpt-4", "original": "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.", "rewritten": "The rapid advancement of Large Language Models (LLMs) in the field of artificial intelligence has enabled them to excel in various tasks, such as document completion and question answering, which may be relevant to researchers in neuroscience studying language processing and comprehension. However, the unregulated use of these models raises concerns about potential negative consequences, such as plagiarism, generating fake news, and spamming. As a result, it is crucial to develop reliable methods for detecting AI-generated text to ensure responsible use of LLMs in interdisciplinary research.\n\nRecent approaches to this problem have focused on identifying unique model signatures within the generated text or applying watermarking techniques that embed specific patterns. In this study, we demonstrate both empirically and theoretically that these detection methods are not reliable in practical situations. We present evidence that paraphrasing attacks, in which a light paraphrasing algorithm is applied to the generated text, can effectively bypass various detectors, including those based on watermarking schemes, neural network-based detectors, and zero-shot classifiers.\n\nFurthermore, we provide a theoretical result suggesting that even the best possible detector for a highly accurate language model can only perform slightly better than a random classifier. This finding may be of interest to neuroscientists examining the limitations of artificial systems in comparison to human language processing abilities. Additionally, we reveal that even LLMs with watermarking protection can be susceptible to spoofing attacks, wherein adversarial humans can deduce hidden watermarking signatures and incorporate them into their own generated text. This could lead to false attribution to the LLMs and potential reputational damage for their developers.\n\nWe hope these findings will spark an open discussion within the research community about the ethical and reliable use of AI-generated text, particularly in the context of interdisciplinary research involving neuroscience and other fields."}
{"paper_sha": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "model": "gpt-4", "original": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.", "rewritten": "We introduce Imagen, a novel text-to-image diffusion model that demonstrates exceptional photorealism and an advanced understanding of language, which may have potential applications in the field of neuroscience, particularly in the study of language processing and visual perception. Imagen leverages the capabilities of large transformer language models, akin to neural networks in the brain, to comprehend text and relies on the robustness of diffusion models for generating high-quality images.\n\nA key finding of our research is that generic large language models, such as T5, pre-trained on text-only datasets, are remarkably efficient at encoding textual information for image synthesis. This suggests that increasing the size of the language model in Imagen significantly improves both the fidelity of the generated images and the alignment between the image and the input text, more so than increasing the size of the image diffusion model. This concept may be of interest to neuroscientists studying the relationship between language processing and visual perception.\n\nImagen sets a new benchmark with a state-of-the-art FID score of 7.27 on the COCO dataset, without any training on COCO, and human evaluators rate Imagen samples as being comparable to the COCO data in terms of image-text alignment. To further evaluate text-to-image models, we propose DrawBench, a comprehensive and rigorous benchmark for assessing text-to-image models. Using DrawBench, we compare Imagen with recent techniques such as VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and discover that human raters favor Imagen over competing models in side-by-side comparisons, in terms of both sample quality and image-text alignment. For a summary of our findings, please visit https://imagen.research.google/."}
{"paper_sha": "98b40eb3ce79c24d9726556947e2e2094737fe46", "model": "gpt-4", "original": "GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.", "rewritten": "GPT series models, including GPT-3, CodeX, InstructGPT, ChatGPT, and others, have recently gained significant interest due to their remarkable abilities in natural language processing, which can be likened to the complex language processing observed in the human brain. However, while there has been considerable research comparing the performance of these models to fine-tuned models, there has been limited examination of how the capabilities of GPT series models have evolved over time. In order to thoroughly investigate the abilities of these models, we selected six representative models, including two from the GPT-3 series (davinci and text-davinci-001) and four from the GPT-3.5 series (code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo).\n\nWe assessed their performance on nine natural language understanding tasks, similar to cognitive tasks in neuroscience research, using 21 datasets. Specifically, we compared the performance and robustness of different models for each task under zero-shot and few-shot scenarios, which can be compared to the way the brain learns and generalizes from limited exposure to new information. Our comprehensive experiments revealed that the overall ability of GPT series models on natural language understanding tasks does not consistently improve as the models evolve, particularly with the introduction of the RLHF training strategy. This strategy, while enhancing the models' ability to generate responses that resemble human-like language processing, also hinders their ability to solve some tasks.\n\nMoreover, our findings suggest that there is still potential for improvement in areas such as model robustness, which could be of interest to neuroscientists studying the resilience and adaptability of human cognitive processes."}
{"paper_sha": "0d25a53184a9c56084416b292de9a8fef4b27347", "model": "gpt-4", "original": null, "rewritten": null}
