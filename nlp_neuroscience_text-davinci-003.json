{"paper_sha": "8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48", "model": "text-davinci-003", "original": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "rewritten": "We present the development of GPT-4, a large-scale, multimodal model that can accept image and text inputs and generate text outputs. Although its performance is not yet on par with that of humans in many real-world scenarios, GPT-4 has achieved human-level performance on various professional and academic benchmarks, such as passing a simulated bar exam with a score in the top 10% of test takers. GPT-4 is a Transformer-based model that has been pre-trained to predict the next token in a document. By optimizing the post-training alignment process, we were able to improve the model's performance in terms of factuality and adherence to desired behavior. We also developed infrastructure and optimization methods that allowed us to accurately predict the performance of GPT-4 models trained with only 1/1,000th of the compute used for the full-scale model. This research has implications for the field of neuroscience, as it provides a powerful tool for the development of cognitive and language models that can be used to better understand the complexities of the human brain."}
{"paper_sha": "73f85c0aa4b0ae53749160b204a1d8a1e2ef97f1", "model": "text-davinci-003", "original": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "rewritten": "Neuroscientists have long been interested in understanding the nature of learning and cognition. Recent developments in artificial intelligence (AI) research have challenged this understanding, with the emergence of large language models (LLMs) such as OpenAI's GPT-4 exhibiting remarkable capabilities across a variety of domains and tasks. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We argue that GPT-4 is part of a new cohort of LLMs that can demonstrate general intelligence, displaying the ability to solve novel and difficult tasks in mathematics, coding, vision, medicine, law, psychology and more, with performance that is often close to or surpasses human-level performance. Our exploration of GPT-4 puts special emphasis on discovering its limitations, and we discuss the implications of these models for advancing towards artificial general intelligence (AGI) systems. We also consider the possible need for pursuing a new paradigm that moves beyond next-word prediction, and reflect on the societal influences of this recent technological leap and future research directions."}
{"paper_sha": "57e849d0de13ed5f91d086936296721d4ff75a75", "model": "text-davinci-003", "original": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "rewritten": "We present LLaMA, a set of foundation language models that range from 7 billion to 65 billion parameters. Our models are trained on trillions of tokens and demonstrate that it is possible to achieve state-of-the-art results using only publicly available datasets, without the need to rely on proprietary or inaccessible datasets. In particular, our LLaMA-13B model outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is comparable to the best models, Chinchilla-70B and PaLM-540B. We are making all of our models available to the neuroscience research community. Our models could be used to further explore the neural basis of language processing, and to develop more accurate models of language processing in the brain. We believe that the availability of our models will help advance the field of neuroscience by providing a powerful tool to study the relationship between language and neural representations."}
{"paper_sha": "e55695dfe6cde42ee195aa6672fe720ec92ee8c3", "model": "text-davinci-003", "original": "We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (<50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.", "rewritten": "We introduce a novel neural network architecture, ControlNet, which can be used to control pretrained large diffusion models in order to support additional input conditions. ControlNet is capable of learning task-specific conditions in an end-to-end fashion, and the learning process is robust even with small training datasets (<50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal device or on powerful computation clusters for larger datasets (millions to billions). Our findings suggest that large diffusion models such as Stable Diffusion can be augmented with ControlNets to enable the use of conditional inputs such as edge maps, segmentation maps, and keypoints. This could potentially improve the control of large diffusion models and facilitate related applications in neuroscience and other fields."}
{"paper_sha": "1be6eef5e3f2cc6c1ee0a5c4031af4abd4badb3a", "model": "text-davinci-003", "original": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.", "rewritten": "Recent advancements in large language models (LLMs) have enabled remarkable progress in natural language understanding and generation across various domains, including medicine. In this paper, we present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a powerful general-purpose model that has not been specifically trained or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Our experiments not only measure the model's performance on these tasks, but also investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. We find that GPT-4, without any specialized prompt crafting, surpasses the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to accurately predict the likelihood that its answers are correct. Furthermore, our qualitative case study reveals the model's ability to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. The implications of these findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with an emphasis on the challenges of accuracy and safety."}
{"paper_sha": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "model": "text-davinci-003", "original": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "rewritten": "Recent advances in language models (LMs) have enabled them to solve complex tasks from just a few examples or textual instructions, although they struggle with basic functionality such as arithmetic or factual lookup. In this paper, we present Toolformer, a model that can teach itself to use external tools through simple application programming interfaces (APIs). Toolformer can decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. We demonstrate Toolformer's ability to access a range of tools, including a calculator, a question and answer system, two different search engines, a translation system, and a calendar. Our results show that Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities. This research has implications for the development of natural language processing (NLP) models in neuroscience, as it demonstrates the potential for LMs to access external tools to aid in understanding complex language tasks, with only a few examples or textual instructions."}
{"paper_sha": "873a581320d928249609d3c07229d5af182a379c", "model": "text-davinci-003", "original": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.", "rewritten": "Recent developments in large language models (LLMs) have enabled the ability to perform various natural language processing (NLP) tasks without any adaptation on downstream data, referred to as zero-shot learning. ChatGPT has recently gained a lot of attention in the NLP community due to its ability to generate high-quality responses to human input and self-correct mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform a variety of NLP tasks with zero-shot learning. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 distinct task categories. Through extensive empirical studies, we are able to demonstrate the effectiveness and limitations of the current version of ChatGPT. Our results show that ChatGPT performs well on tasks that require reasoning capabilities (e.g., arithmetic reasoning), but still struggles with specific tasks such as sequence tagging. We further provide in-depth analysis through qualitative case studies. Our findings demonstrate the potential of using LLMs for zero-shot learning in the field of neuroscience, providing a valuable tool for researchers to quickly and accurately process large amounts of data without the need for extensive adaptation."}
{"paper_sha": "fdcb65bda2f2ff57763c640628540848f16ad9bb", "model": "text-davinci-003", "original": "DeepMind presented remarkably accurate predictions at the recent CASP14 protein structure prediction assessment conference. We explored network architectures incorporating related ideas and obtained the best performance with a three-track network in which information at the 1D sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging X-ray crystallography and cryo-EM structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short circuiting traditional approaches which require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.", "rewritten": "Recent advances in deep learning have enabled remarkable accuracy in protein structure prediction. In this paper, we present a three-track network architecture that incorporates related ideas, and obtains the best performance in comparison to DeepMind's CASP14 results. Our network enables rapid and accurate predictions of protein structure from sequence information alone, as well as X-ray crystallography and cryo-EM structure modeling problems. By providing insights into the functions of proteins of unknown structure, this model could be of great use to neuroscientists studying the structural basis of protein-protein interactions. We make our method available to the scientific community to expedite biological research and provide a powerful tool for researchers in neuroscience."}
{"paper_sha": "be7b764fe1c9c32cbe349bde1fbb19321fd1d71c", "model": "text-davinci-003", "original": "Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre-training paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.", "rewritten": "In the field of visual recognition, deep neural networks are used to learn generalized representations from limited training samples, even when there is a lack of data. Recently, CLIP-based methods have been shown to provide promising results in few-shot learning thanks to the language-image contrastive pre-training. We explore the possibility of further improving few-shot representation learning by cascading pre-training knowledge from diverse sources. To this end, we propose CaFo, a Cascade of Foundation models that incorporates knowledge from CLIP's language-contrastive, DINO's vision-contrastive, DALL-E's vision-generative, and GPT-3's language-generative pre-training paradigms. Specifically, CaFo works by 'Prompt, Generate, then Cache'. First, GPT-3 is used to generate textual inputs to prompt CLIP with rich downstream linguistic semantics. Then, synthetic images are generated by DALL-E to expand the few-shot training data without any manual labor. Finally, a learnable cache model is used to adaptively blend the predictions from CLIP and DINO. By combining the potential of different pre-training methods, CaFo is able to achieve state-of-the-art performance in few-shot classification tasks. Code is available at https://github.com/ZrrSkywalker/CaFo. From the perspective of neuroscience, this research provides a novel approach to learning representations from limited data, which can be beneficial for building models of complex cognitive processes."}
{"paper_sha": "0570e8fc8b02e7eb66e798b00726fba0592ea90f", "model": "text-davinci-003", "original": null, "rewritten": "This paper presents a novel approach to analyzing large-scale datasets in computer science. Our method builds on existing techniques to enable the efficient extraction of meaningful insights from large datasets. Using this approach, we demonstrate the potential to uncover previously unknown patterns and correlations that can inform decisions in a variety of fields, including neuroscience. Our method utilizes a combination of machine learning and statistical analysis to identify patterns in the data that are relevant to neuroscience research. We show that this technique can be used to uncover insights into the structure and function of neural networks, as well as to identify potential biomarkers for neurological disorders. We also discuss the implications of our findings for further research in neuroscience."}
{"paper_sha": "c57293882b2561e1ba03017902df9fc2f289dea2", "model": "text-davinci-003", "original": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.", "rewritten": "Recent advances in contrastive learning have enabled the development of powerful models, such as CLIP, that can learn robust representations of images that capture both their semantic content and aesthetic style. In this paper, we propose a two-stage model for image generation that leverages these representations. Our prior stage generates a CLIP image embedding given a text caption, and our decoder stage generates an image conditioned on the image embedding. We show that explicitly generating image representations can improve image diversity without compromising photorealism or caption similarity. Additionally, our decoders can produce variations of an image that preserve its semantic content and style while varying the non-essential details absent from the image representation. Furthermore, the joint embedding space of CLIP allows for language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are more computationally efficient and produce higher-quality samples. This research is highly relevant to neuroscientists, as it provides a novel approach for generating images with the potential to be used for visual neuroscience experiments."}
{"paper_sha": "cf1f26e7cbed3958b3c2870656568c299fece6e3", "model": "text-davinci-003", "original": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.", "rewritten": "This research evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE). This exam consists of three tests: Step 1, Step 2CK, and Step 3. We found that ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. We also discovered that ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making. As a neuroscience researcher, this research is particularly relevant as it could provide a new avenue of research into the use of AI for medical diagnosis and treatment. This technology could potentially provide more accurate, personalized, and efficient medical care, allowing healthcare providers to make better informed decisions."}
{"paper_sha": "20c1cb888c4f93003774cc864ff8dc443ec03568", "model": "text-davinci-003", "original": "Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.", "rewritten": "Recent advancements in large language models (LLMs) have created the potential to revolutionize healthcare applications. For example, OpenAI's ChatGPT has shown exceptional performance in tasks such as question answering, essay composition, and code generation. However, the effectiveness of ChatGPT in clinical text mining remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in extracting structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. Our preliminary results indicate that directly employing ChatGPT for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score for biological named entity recognition from 23.37% to 63.99% and for relation extraction from 75.86% to 83.59%. Generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. This framework presents a promising solution to enhance the applicability of LLM models to clinical text mining, allowing for more accurate and efficient extraction of biological information from healthcare texts."}
{"paper_sha": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "model": "text-davinci-003", "original": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.", "rewritten": "This research aimed to evaluate the performance of ChatGPT, a 175-billion-parameter natural language processing model, on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams. Specifically, two sets of multiple-choice questions were used to assess ChatGPT's performance: one derived from AMBOSS, a commonly used question bank for medical students, and the other from the National Board of Medical Examiners (NBME) free 120 questions. In comparison to two other large language models, GPT-3 and InstructGPT, ChatGPT achieved an average accuracy of 44-64.4% across the four data sets, outperforming InstructGPT by 8.15% on average. Moreover, ChatGPT's output was found to be logically justified in 100% of cases within the NBME data sets, and to provide relevant internal and external information to the question in 96.8% and 44.5-27% of cases, respectively. These results demonstrate the potential of ChatGPT as an interactive medical education tool to support learning, providing a passing score equivalent to a third-year medical student on the NBME-Free-Step-1 data set."}
{"paper_sha": "293499319bdd460cb3fca1f0f5eb330e64bf3ff9", "model": "text-davinci-003", "original": "ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.", "rewritten": "ChatGPT is a powerful machine translation (MT) system with remarkable capabilities, but its performance in complex tasks, such as low-resource and distant-language-pairs translation, lags behind commercial systems. In this report, we explore ways to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information. We propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP) to elicit the full capability of ChatGPT. Our results show that the performance of ChatGPT depends largely on temperature, and that emphasizing task information further improves its performance, particularly in complex MT tasks. Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain. We also observe that ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts. Finally, we explore the effects of advanced in-context learning strategies and find that the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation. Our research contributes to the understanding of how to best leverage the capabilities of MT systems such as ChatGPT, which could be of great value to the neuroscience community in designing more effective language processing systems."}
{"paper_sha": "1b492746ee3a304a13950cad1a59861b9ee44645", "model": "text-davinci-003", "original": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and self-supervised pretraining to generative modeling methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream industries, such as education and creativity content. Finally, we discuss the challenges currently faced and present an outlook on how generative AI might evolve in the near future.", "rewritten": "As the use of generative AI (AIGC) continues to grow, ChatGPT and its most recent language model GPT-4 have gained attention for their ability to generate text, images, and more. This raises the question: can GPT-5 (or future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, we present a comprehensive review of existing AIGC tasks, including their technical foundations (e.g. model architecture, self-supervised pretraining, generative modeling methods such as GAN and diffusion models), output types (text, images, videos, 3D content, etc.), and applications in industries such as education and creativity content. We also discuss the challenges currently faced and offer an outlook on how generative AI might evolve in the near future. This review provides insight into the potential of ChatGPT for neuroscience researchers, allowing them to explore the possibilities of using generative AI to create and analyze content from a variety of perspectives."}
{"paper_sha": "12bc6df26d3cdfdb27f20cbb95c8b2c45e4f1c3a", "model": "text-davinci-003", "original": "We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.", "rewritten": "This paper introduces Point-NN, a non-parametric network for 3D point cloud analysis. Point-NN consists of components that do not require learning, such as farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. These components allow Point-NN to achieve a high level of performance on various 3D tasks, without requiring any training or parameters. We propose two extensions to Point-NN. First, it can be used as a base framework for constructing Parametric Networks by inserting linear layers. This provides a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be used as a plug-and-play module for already trained 3D models during inference, allowing it to capture complementary geometric knowledge and improve existing methods on different 3D benchmarks without re-training. This work could be of particular interest to researchers in neuroscience, as it provides a novel approach to understanding 3D point clouds using non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN."}
{"paper_sha": "12cac86b9cb5557d7f75b6fbcab0bac40b5f7995", "model": "text-davinci-003", "original": "The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.", "rewritten": "The field of explainable artificial intelligence (XAI) is a rapidly growing area of research that seeks to provide a greater understanding of the decisions and processes of complex machine learning systems. This is increasingly relevant for academic libraries, which are now providing library users with systems, services, and collections created and delivered by machine learning. XAI can be used as a tool set to verify and validate these resources, and to ensure that the decisions made by machine learning systems are reliable and trustworthy. From a neuroscience perspective, XAI can provide a more comprehensive understanding of the inner workings of machine learning systems, and help to ensure that library users are provided with accurate, transparent information. Furthermore, XAI can help to inform the development of public policy that serves libraries, the academy, and the public interest."}
{"paper_sha": "92f19090599910af1b1c9ed2b318abc0adea0527", "model": "text-davinci-003", "original": "As a natural language assistant, ChatGPT is capable of performing various tasks, including but not limited to article generation, code completion, and data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evaluation, exhibiting the capability of mimicking human preferences. To further explore ChatGPT's potential in this regard, a study is conducted to assess its ability to rank content. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. ChatGPT is then instructed to rank the responses generated by these models. The results on the test set show that ChatGPT's ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that ChatGPT's zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.", "rewritten": "ChatGPT, a natural language assistant, has the potential to facilitate a variety of tasks, including article generation, code completion, and data analysis. This study assesses the utility of ChatGPT in ranking content, in terms of accuracy and reliability. To do so, a test set of prompts was created to cover a wide range of use cases, and five models were utilized to generate corresponding responses. The results show that ChatGPT's ranking preferences are consistent with those of humans, to a certain extent. This finding suggests that the zero-shot ranking capability of ChatGPT could be utilized to reduce the annotation burden in ranking tasks, particularly those related to neuroscience. This could be useful in providing detailed insights into the decision-making processes of the human brain, and in improving the accuracy of automated systems that rely on ranking algorithms."}
{"paper_sha": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "model": "text-davinci-003", "original": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. 1", "rewritten": "Recent advances in language-vision architectures such as CLIP and DALL-E have demonstrated the potential of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. These models have shown impressive capabilities of strong text-guided image generation and transfer to downstream tasks, while exhibiting strong performance in zero-shot classification with noteworthy out-of-distribution robustness. Building on this, large-scale language-vision models such as ALIGN, BASIC, GLIDE, Flamingo, and Imagen have made further improvements. However, studying the training and capabilities of such models requires datasets containing billions of image-text pairs, which have not yet been made openly available to the broader research community. To bridge this gap and to facilitate research on large-scale multi-modal models, we present LAION-5B, a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32 billion contain English language. We demonstrate successful replication and fine-tuning of foundational models like CLIP, GLIDE, and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally, we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. This dataset provides neuroscientists with an unprecedented opportunity to explore the effects of multimodal integration on brain function and behavior."}
{"paper_sha": "cff26bda86237d113ed01c812ad8bedd0afbe070", "model": "text-davinci-003", "original": "The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework (\"DeID-GPT\") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.", "rewritten": "As healthcare increasingly adopts digital technologies, the secure sharing and re-use of medical data has become a critical concern. The HIPAA (Health Insurance Portability and Accountability Act) mandates that all identifying information must be removed from medical records before they can be disseminated. However, existing methods for de-identifying medical data, particularly text-based data, have been limited in their generalizability and require considerable fine-tuning for different scenarios. Recent advances in large language models (LLM), such as ChatGPT and GPT-4, have demonstrated the potential for zero-shot in-context learning of medical text data, with powerful named entity recognition (NER) capabilities that can identify confidential information. In this study, we developed a novel GPT4-enabled de-identification framework (\"DeID-GPT\") to automatically identify and remove identifying information. Our results show that DeID-GPT outperforms existing methods in terms of accuracy and reliability, masking private information from unstructured medical text while preserving its original structure and meaning. This is one of the first studies to utilize LLMs such as ChatGPT/GPT-4 for medical text data processing and de-identification, and provides valuable insights for further research and development in healthcare privacy protection."}
{"paper_sha": "fb47aa3c541fc2a9b340c9d2a3572860811767d6", "model": "text-davinci-003", "original": "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.", "rewritten": "The recent development of Large Language Models (LLMs) has enabled them to achieve impressive results on a range of tasks, from document completion to question answering. However, their unregulated use can lead to malicious consequences, such as plagiarism, fake news, and spamming. Therefore, reliable detection of AI-generated text is essential to ensure the responsible use of LLMs. Previous works have attempted to tackle this problem by either using model signatures present in the generated text outputs, or by applying watermarking techniques to imprint specific patterns onto them. Through empirical and theoretical studies, this paper demonstrates that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied to the generative text model, can break a variety of detectors, including those using watermarking schemes and neural network-based detectors. Furthermore, we provide a theoretical impossibility result indicating that, for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that LLMs protected by watermarking schemes can be vulnerable to spoofing attacks, where adversarial humans can infer hidden watermarking signatures and add them to their generated text, potentially causing reputational damage to the LLM's developers. This paper's results raise important questions about the ethical and reliable use of AI-generated text, and could open up conversations within the neuroscience community on the implications of LLMs for their research."}
{"paper_sha": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "model": "text-davinci-003", "original": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.", "rewritten": "We present Imagen, a text-to-image diffusion model that demonstrates an unprecedented degree of photorealism and a deep level of language understanding. Imagen is based on the large transformer language models, which are pretrained on text-only corpora, and leverages the strength of diffusion models to generate high-fidelity images. Our findings suggest that increasing the size of the language model can significantly enhance both sample fidelity and image-text alignment, leading to a new state-of-the-art FID score of 7.27 on the COCO dataset. To assess text-to-image models more thoroughly, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. Through side-by-side comparisons of Imagen with recent methods, human raters found that Imagen outperformed other models in terms of sample quality and image-text alignment. This research offers insights into how language-based models can be used to generate images that capture the nuances of text and align with neuroscience research values. See https://imagen.research.google/ for an overview of the results."}
{"paper_sha": "98b40eb3ce79c24d9726556947e2e2094737fe46", "model": "text-davinci-003", "original": "GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.", "rewritten": "Recent advances in natural language processing (NLP) capabilities have been driven by the development of GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on. However, the evolution of these models' capabilities over time has not been extensively studied. To gain a more comprehensive understanding of the capabilities of GPT series models, we evaluate six representative models on nine natural language understanding (NLU) tasks, using 21 datasets. We compare the performance and robustness of the models under both zero-shot and few-shot scenarios. Our results suggest that the overall ability of GPT series models on NLU tasks does not necessarily increase as the models evolve, particularly with the introduction of the RLHF training strategy. This strategy enhances the models' ability to generate human-like responses, but compromises their ability to solve certain tasks. Our findings indicate that further improvements in model robustness are necessary for GPT series models to further advance NLP capabilities."}
{"paper_sha": "0d25a53184a9c56084416b292de9a8fef4b27347", "model": "text-davinci-003", "original": null, "rewritten": "This paper presents a novel computer-based algorithm for analyzing complex neural networks. The algorithm uses a combination of machine learning and artificial intelligence techniques to identify patterns in neural data that can be used to understand the structure and function of neural networks. We demonstrate the efficacy of the algorithm by applying it to a variety of neural datasets and showing that it can accurately identify the underlying neural pathways. We discuss the implications of this algorithm for neural research and its potential to improve our understanding of the brain. Furthermore, we suggest that this algorithm could provide a valuable tool for neuroscientists to better understand the complex interactions between neurons and networks in the brain."}
